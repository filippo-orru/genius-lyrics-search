{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qPH1B4eooUI",
        "outputId": "b371491e-8668-4b07-e20a-9df5525f008a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting swifter\n",
            "  Downloading swifter-1.4.0.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from swifter) (1.5.3)\n",
            "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.10/dist-packages (from swifter) (5.9.5)\n",
            "Requirement already satisfied: dask[dataframe]>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from swifter) (2023.8.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (23.2)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (1.4.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (6.0.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (0.12.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (7.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter) (1.23.5)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2023.11.17)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[dataframe]>=2.10.0->swifter) (3.17.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.2.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.6)\n",
            "Building wheels for collected packages: swifter\n",
            "  Building wheel for swifter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swifter: filename=swifter-1.4.0-py3-none-any.whl size=16506 sha256=b29da3082ef6f6fb6e907d8ed4b5c23eb14317e04e07f0e8af5da0068688af91\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/cf/51/0904952972ee2c7aa3709437065278dc534ec1b8d2ad41b443\n",
            "Successfully built swifter\n",
            "Installing collected packages: opendatasets, swifter\n",
            "Successfully installed opendatasets-0.1.22 swifter-1.4.0\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk swifter opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BprIAHtdooUL",
        "ExecuteTime": {
          "end_time": "2024-01-08T21:19:44.345197Z",
          "start_time": "2024-01-08T21:19:44.341216Z"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import string\n",
        "import swifter\n",
        "import torch\n",
        "import opendatasets as od\n",
        "import torch.nn as nn\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "vs7O_kVkooUM"
      },
      "source": [
        "# Download Data\n",
        "\n",
        "1. Option: download the full dataset and select some percentage. You will need to enter your username and kaggle API token when prompted. You can create an API token at https://www.kaggle.com > settings > API > Create New API Token.\n",
        "\n",
        "2. Option: load preprocessed data and qrels (you can skip the section for preprocessing document and generating queries below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaKJpM32ooUO",
        "outputId": "ec578398-95fc-4d9d-dfed-68eed10a4755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: "
          ]
        }
      ],
      "source": [
        "# OPTION 1\n",
        "dataset = 'https://www.kaggle.com/datasets/carlosgdcj/genius-song-lyrics-with-language-information/'\n",
        "# Download the data set using opendatasets\n",
        "od.download(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93dOtdVwooUP"
      },
      "outputs": [],
      "source": [
        "# We can't work with a 9GB file, so read 10% of the data at random\n",
        "p = 0.1\n",
        "# keep the header, then take only p% of lines\n",
        "df = pd.read_csv(\n",
        "    \"./genius-song-lyrics-with-language-information/song_lyrics.csv\",\n",
        "    header=0,\n",
        "    skiprows=lambda i: i > 0 and random.random() > p\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T12:31:34.470187Z",
          "start_time": "2024-01-08T12:31:34.373535Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL4eyrbeooUQ",
        "outputId": "62cb8fda-98e0-4276-853e-a9ea26d137a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('qrels-test.pkl', <http.client.HTTPMessage at 0x7af5311f24a0>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# OPTION 2\n",
        "import urllib.request\n",
        "\n",
        "urllib.request.urlretrieve('https://f002.backblazeb2.com/file/ffactory-public/documents.pkl', 'documents.pkl')\n",
        "urllib.request.urlretrieve('https://f002.backblazeb2.com/file/ffactory-public/qrels-train.pkl', 'qrels-train.pkl')\n",
        "urllib.request.urlretrieve('https://f002.backblazeb2.com/file/ffactory-public/qrels-test.pkl', 'qrels-test.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rB_0XfynooUS"
      },
      "outputs": [],
      "source": [
        "df = torch.load('documents.pkl')\n",
        "qrels_train = torch.load('qrels-train.pkl')\n",
        "qrels_test = torch.load('qrels-test.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.reset_index()"
      ],
      "metadata": {
        "id": "Pv2IQGXHriiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI0yGRD5ooUT"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-07T10:00:04.750355Z",
          "start_time": "2024-01-07T10:00:04.744511Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGTLDeurooUT",
        "outputId": "d1bb159b-6eda-4b41-bf58-e28edf8f571c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-07T10:17:54.823173Z",
          "start_time": "2024-01-07T10:17:53.241227Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "OiF1Z3paooUV",
        "outputId": "1cb5b443-e730-4dae-8953-89d468fbc21a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'language'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'language'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9e95da413eb4>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Select only english songs and songs which contain verses (newlines)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"language\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lyrics\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'language'"
          ]
        }
      ],
      "source": [
        "ps = PorterStemmer()\n",
        "stopwords_en = set(stopwords.words('english'))\n",
        "\n",
        "# Select only english songs and songs which contain verses (newlines)\n",
        "df = df[df[\"language\"] == \"en\"]\n",
        "df = df[df[\"lyrics\"].str.contains(\"\\n\")]\n",
        "\n",
        "# Select columns we care about\n",
        "df = df[[\"title\", \"lyrics\", \"views\", \"id\"]]\n",
        "df.rename(columns={\"id\": \"doc_id\"}, inplace=True)\n",
        "df.set_index(\"doc_id\", inplace=True)\n",
        "\n",
        "df_proc = df.copy()\n",
        "df_proc.rename(columns={\"lyrics\": \"text\"}, inplace=True)\n",
        "\n",
        "# Convert to lowercase\n",
        "df_proc[\"text\"] = df_proc[\"text\"].str.lower()\n",
        "# Remove any non-alphanumeric / whitespace characters\n",
        "df_proc[\"text\"] = df_proc[\"text\"].str.replace(re.compile(r\"[^\\w\\s]\"), \"\", regex=True)\n",
        "# Remove newlines\n",
        "df_proc[\"text\"] = df_proc[\"text\"].str.replace(\"\\n\", \" \", regex=False)\n",
        "# Remove text between square brackets\n",
        "df_proc[\"text\"] = df_proc[\"text\"].str.replace(re.compile(r\"\\[.{0,100}\\]\"), \"\", regex=True)\n",
        "# Split text into words\n",
        "df_proc[\"text\"] = df_proc[\"text\"].str.rsplit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-07T10:00:01.453221Z",
          "start_time": "2024-01-07T10:00:01.430381Z"
        },
        "id": "uw-hAgqJooUW"
      },
      "outputs": [],
      "source": [
        "print(f\"number of english songs: {len(df_proc)}\")\n",
        "df_proc.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-07T10:00:47.261930Z",
          "start_time": "2024-01-07T10:00:11.500207Z"
        },
        "id": "AXovp3KIooUX"
      },
      "outputs": [],
      "source": [
        "# Remove stopwords and stem tokens\n",
        "def remove_stopwords_and_stem(tokens):\n",
        "    return \" \".join([ps.stem(token) for token in tokens if token not in stopwords_en])\n",
        "\n",
        "\n",
        "df_proc[\"text\"] = df_proc[\"text\"].swifter.apply(remove_stopwords_and_stem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-07T10:00:51.886296Z",
          "start_time": "2024-01-07T10:00:51.862829Z"
        },
        "id": "Y985jYZGooUX"
      },
      "outputs": [],
      "source": [
        "# Compare the original lyrics with the tokenized lyrics\n",
        "(df.iloc[5][\"lyrics\"][0:100], df_proc.iloc[5][\"text\"][0:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-07T10:00:56.242240Z",
          "start_time": "2024-01-07T10:00:56.201640Z"
        },
        "id": "uCwO53WDooUY",
        "outputId": "10495179-485b-4d64-e6e2-bec6dc1abfe5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>views</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Killa Cam</td>\n",
              "      <td>173166</td>\n",
              "      <td>choru opera steve camron killa cam killa cam c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Can I Live</td>\n",
              "      <td>468624</td>\n",
              "      <td>produc irv gotti intro yeah hah yeah rocafella...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Forgive Me Father</td>\n",
              "      <td>4743</td>\n",
              "      <td>mayb caus im eatin bastard fiend grub carri pu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Down and Out</td>\n",
              "      <td>144404</td>\n",
              "      <td>produc kany west brian miller intro camron kan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Fly In</td>\n",
              "      <td>78271</td>\n",
              "      <td>intro ask young boy gon second time around gon...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               title   views  \\\n",
              "0          Killa Cam  173166   \n",
              "1         Can I Live  468624   \n",
              "2  Forgive Me Father    4743   \n",
              "3       Down and Out  144404   \n",
              "4             Fly In   78271   \n",
              "\n",
              "                                                text  \n",
              "0  choru opera steve camron killa cam killa cam c...  \n",
              "1  produc irv gotti intro yeah hah yeah rocafella...  \n",
              "2  mayb caus im eatin bastard fiend grub carri pu...  \n",
              "3  produc kany west brian miller intro camron kan...  \n",
              "4  intro ask young boy gon second time around gon...  "
            ]
          },
          "execution_count": 189,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_proc.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-07T10:01:03.954364Z",
          "start_time": "2024-01-07T10:01:03.704188Z"
        },
        "id": "aCblzrMBooUY",
        "outputId": "633acc80-2e6c-48ab-b31e-c296ae5f9c94"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Killa Cam</td>\n",
              "      <td>choru opera steve camron killa cam killa cam c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Can I Live</td>\n",
              "      <td>produc irv gotti intro yeah hah yeah rocafella...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Forgive Me Father</td>\n",
              "      <td>mayb caus im eatin bastard fiend grub carri pu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Down and Out</td>\n",
              "      <td>produc kany west brian miller intro camron kan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Fly In</td>\n",
              "      <td>intro ask young boy gon second time around gon...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               title                                               text\n",
              "0          Killa Cam  choru opera steve camron killa cam killa cam c...\n",
              "1         Can I Live  produc irv gotti intro yeah hah yeah rocafella...\n",
              "2  Forgive Me Father  mayb caus im eatin bastard fiend grub carri pu...\n",
              "3       Down and Out  produc kany west brian miller intro camron kan...\n",
              "4             Fly In  intro ask young boy gon second time around gon..."
            ]
          },
          "execution_count": 190,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save as a pickle file\n",
        "torch.save(df_proc[[\"title\", \"text\"]], 'documents.pkl')\n",
        "\n",
        "# Test if the pickle file is saved correctly\n",
        "df_reloaded = torch.load('documents.pkl')\n",
        "df_reloaded.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W13tBCYxooUZ"
      },
      "source": [
        "# Generate Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-07T10:01:24.590311Z",
          "start_time": "2024-01-07T10:01:24.579912Z"
        },
        "id": "aquMJ97VooUZ"
      },
      "outputs": [],
      "source": [
        "# Functions to Select Verses\n",
        "def getFirstVerses(lyricsString, amount):\n",
        "    verseList = re.split('\\n', lyricsString)\n",
        "    FinalList = [i for i in verseList if (len(i) > 1 and i[0] != '[')]\n",
        "    return \" \".join(FinalList[:amount])\n",
        "\n",
        "\n",
        "def getFirstVersesOfChorus(lyricsString, amount):\n",
        "    List = re.split('\\n', lyricsString)\n",
        "    verseList = [i for i in List if len(i) > 1]\n",
        "    for i in range(len(verseList)):\n",
        "        if \"[Chorus\" in verseList[i] or \"[Hook\" in verseList[i]:\n",
        "            return \" \".join(verseList[i + 1:i + amount + 1])\n",
        "    return getFirstVerses(lyricsString, amount)\n",
        "\n",
        "\n",
        "def getRandomVerses(lyricsString, amount):\n",
        "    verseList = re.split('\\n', lyricsString)\n",
        "    FinalList = [i for i in verseList if (len(i) > 1 and i[0] != '[')]\n",
        "    rd = random.randint(0, len(FinalList) - amount)\n",
        "    return \" \".join(FinalList[rd:rd + amount])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-07T10:02:13.193171Z",
          "start_time": "2024-01-07T10:02:13.185882Z"
        },
        "id": "xTPkxtjrooUa"
      },
      "outputs": [],
      "source": [
        "# Functions to Degrade message\n",
        "\n",
        "#Function to create typo by neighbouring letter\n",
        "NeighbouringKeys = {\n",
        "    'q': \"qwas\",\n",
        "    'w': \"qwase\",\n",
        "    'e': \"wsedr\",\n",
        "    'r': \"edrft\",\n",
        "    't': \"rftgy\",\n",
        "    'y': \"tgyhu\",\n",
        "    'u': \"yhuji\",\n",
        "    'i': \"ujiko\",\n",
        "    'o': \"ikolp\",\n",
        "    'p': \"olp\",\n",
        "    'a': \"qwasz\",\n",
        "    's': \"wazsxed\",\n",
        "    'd': \"sxedcrf\",\n",
        "    'f': \"dcrfvtg\",\n",
        "    'g': \"fvtgbyh\",\n",
        "    'h': \"gbyhnuj\",\n",
        "    'j': \"hnujmik\",\n",
        "    'k': \"jmikol\",\n",
        "    'l': \"kolp\",\n",
        "    'z': \"azsx\",\n",
        "    'x': \"zsxdc\",\n",
        "    'c': \"xdcfv\",\n",
        "    'v': \"cfvgb\",\n",
        "    'b': \"vgbhn\",\n",
        "    'n': \"bhnjm\",\n",
        "    'm': \"njmk\"\n",
        "}\n",
        "\n",
        "englishLetters = NeighbouringKeys.keys()\n",
        "\n",
        "\n",
        "def typos(text, prob=0.01):\n",
        "    resultingText = \"\"\n",
        "\n",
        "    for letter in text:\n",
        "        if not letter in englishLetters:\n",
        "            newLetter = letter\n",
        "        else:\n",
        "            if random.random() < prob:\n",
        "                newLetter = random.choice(NeighbouringKeys[letter])\n",
        "            else:\n",
        "                newLetter = letter\n",
        "        resultingText += newLetter\n",
        "\n",
        "    return resultingText\n",
        "\n",
        "\n",
        "#Function to (maybe) invert 2 adjacent letters (do force=True to force it to happen)\n",
        "def invertAdjacentLetters(text, force=False):\n",
        "    rd = random.randint(0, len(text) - 2)\n",
        "    if not force:\n",
        "        if text[rd] in englishLetters and text[rd + 1] in englishLetters:\n",
        "            return text[:rd] + text[rd + 1] + text[rd] + text[rd + 2:]\n",
        "        else:\n",
        "            return text\n",
        "    else:\n",
        "        while not (text[rd] in englishLetters and text[rd + 1] in englishLetters):\n",
        "            rd = random.randint(0, len(text) - 2)\n",
        "        return text[:rd] + text[rd + 1] + text[rd] + text[rd + 2:]\n",
        "\n",
        "\n",
        "#Function to (maybe) remove a letter (do force=True to force it to happen)\n",
        "def removeLetter(text, force=False):\n",
        "    rd = random.randint(0, len(text) - 1)\n",
        "    if not force:\n",
        "        if text[rd] in englishLetters:\n",
        "            return text[:rd] + text[rd + 1:]\n",
        "        else:\n",
        "            return text\n",
        "    else:\n",
        "        while not (text[rd] in englishLetters):\n",
        "            rd = random.randint(0, len(text) - 1)\n",
        "        return text[:rd] + text[rd + 1:]\n",
        "\n",
        "\n",
        "#Function to (maybe) double a letter (do force=True to force it to happen)\n",
        "def doubleLetter(text, force=False):\n",
        "    rd = random.randint(0, len(text) - 1)\n",
        "    if not force:\n",
        "        if text[rd] in englishLetters:\n",
        "            return text[:rd + 1] + text[rd] + text[rd + 1:]\n",
        "        else:\n",
        "            return text\n",
        "    else:\n",
        "        while not (text[rd] in englishLetters):\n",
        "            rd = random.randint(0, len(text) - 1)\n",
        "        return text[:rd + 1] + text[rd] + text[rd + 1:]\n",
        "\n",
        "\n",
        "CommonMisspelling = {\n",
        "    \"absence\": [\"absense\", \"absentse\", \"abcense\", \"absance\"],\n",
        "    \"acceptable\": [\"acceptible\"],\n",
        "    \"their\": [\"there\", \"they're\"],\n",
        "    \"there\": [\"their\", \"they're\"],\n",
        "    \"they're\": [\"their\", \"there\"],\n",
        "    \"your\": [\"you're\"],\n",
        "    \"you're\": [\"your\"]\n",
        "}\n",
        "\n",
        "\n",
        "# Add a common misspelling\n",
        "def addCommonMisspell(text):\n",
        "    for word in CommonMisspelling.keys():\n",
        "        if word in text:\n",
        "            return text.replace(word, random.choice(CommonMisspelling[word]))\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-07T10:28:55.358663Z",
          "start_time": "2024-01-07T10:28:55.337489Z"
        },
        "id": "uHQqsIERooUa"
      },
      "outputs": [],
      "source": [
        "def generate_qrels(df, n):\n",
        "    # Add 'weight' column\n",
        "    max_views = max(df[\"views\"])\n",
        "    df[\"weight\"] = (df[\"views\"] / max_views) ** 0.5 * 0.5 + 0.1\n",
        "\n",
        "    df_sampled = df.sample(n // 2, weights='weight')\n",
        "\n",
        "    def generate_positive_qrel(document):\n",
        "        text = document['lyrics']\n",
        "        rd = random.random()\n",
        "        if rd < 0.6:\n",
        "            query = getFirstVersesOfChorus(text, random.randint(1, 2))\n",
        "        elif rd < 0.9:\n",
        "            query = getFirstVerses(text, random.randint(1, 2))\n",
        "        else:\n",
        "            query = getRandomVerses(text, random.randint(1, 2))\n",
        "\n",
        "        if random.randint(0, 3) == 0:\n",
        "            query = addCommonMisspell(query)\n",
        "        query = typos(query)\n",
        "\n",
        "        for j in range(len(query)):\n",
        "            rand = random.randint(0, 50)\n",
        "            if rand == 0:\n",
        "                query = invertAdjacentLetters(query)\n",
        "            elif rand == 1:\n",
        "                query = removeLetter(query)\n",
        "            if rand == 2:\n",
        "                query = doubleLetter(query)\n",
        "\n",
        "        doc_id = document.name\n",
        "        return pd.Series([query, doc_id, 1], index=['text', 'doc_id', 'relevance'])\n",
        "\n",
        "    def generate_negative_qrel(positive_qrel):\n",
        "        original_doc_id = positive_qrel['doc_id']\n",
        "        negative_doc_id = original_doc_id\n",
        "        while negative_doc_id == original_doc_id:\n",
        "            negative_doc_id = df.sample(1).iloc[0].name\n",
        "\n",
        "        return pd.Series([positive_qrel['text'], negative_doc_id, 0])\n",
        "\n",
        "    positive_qrels = df_sampled.apply(generate_positive_qrel, axis=1, result_type='expand')\n",
        "    negative_qrels = positive_qrels.apply(generate_negative_qrel, axis=1, result_type='broadcast')\n",
        "\n",
        "    return pd.concat([positive_qrels, negative_qrels]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-07T10:28:57.706140Z",
          "start_time": "2024-01-07T10:28:57.324998Z"
        },
        "id": "1DiI2QWIooUa"
      },
      "outputs": [],
      "source": [
        "# Save qrels as pickle file\n",
        "total_qrels = 1000\n",
        "#total_qrels = 100000\n",
        "qrels_train = generate_qrels(df, int(total_qrels * 0.9))\n",
        "torch.save(qrels_train, 'qrels-train.pkl')\n",
        "\n",
        "qrels_test = generate_qrels(df, int(total_qrels * 0.1))\n",
        "torch.save(qrels_test, 'qrels-test.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "INFH86VhooUb",
        "outputId": "b6b1b4e5-34ab-4911-a1d6-bad51d941421"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text   doc_id relevance\n",
              "0                       Yo Interrnatinoal thuggin' nigga  5274699         1\n",
              "1                                        A lnog time ago  1602274         1\n",
              "2      In thw night time, keep me ou of sight, it's t...  2416243         1\n",
              "3      2015 wrokig at a museum I heard the words from...  6846275         1\n",
              "4                         Now I will burnn before I sing  1749958         1\n",
              "...                                                  ...      ...       ...\n",
              "44995                                              Hook:  7519925         0\n",
              "44996  Like when I talk my shit you don’t ever hear m...  5103921         0\n",
              "44997             You make me dreqm in color, ooh, oh-oh  5417288         0\n",
              "44998                                 (Frennvh Dialogue)  6761337         0\n",
              "44999  If I clud cattch a star befor t touched the gr...  1848781         0\n",
              "\n",
              "[45000 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-743cb263-c08d-4114-aabf-ccd0f93da437\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>doc_id</th>\n",
              "      <th>relevance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Yo Interrnatinoal thuggin' nigga</td>\n",
              "      <td>5274699</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A lnog time ago</td>\n",
              "      <td>1602274</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In thw night time, keep me ou of sight, it's t...</td>\n",
              "      <td>2416243</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2015 wrokig at a museum I heard the words from...</td>\n",
              "      <td>6846275</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Now I will burnn before I sing</td>\n",
              "      <td>1749958</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44995</th>\n",
              "      <td>Hook:</td>\n",
              "      <td>7519925</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44996</th>\n",
              "      <td>Like when I talk my shit you don’t ever hear m...</td>\n",
              "      <td>5103921</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44997</th>\n",
              "      <td>You make me dreqm in color, ooh, oh-oh</td>\n",
              "      <td>5417288</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44998</th>\n",
              "      <td>(Frennvh Dialogue)</td>\n",
              "      <td>6761337</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44999</th>\n",
              "      <td>If I clud cattch a star befor t touched the gr...</td>\n",
              "      <td>1848781</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45000 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-743cb263-c08d-4114-aabf-ccd0f93da437')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-743cb263-c08d-4114-aabf-ccd0f93da437 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-743cb263-c08d-4114-aabf-ccd0f93da437');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2c69c042-58a4-446e-84be-964c4ad4b844\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2c69c042-58a4-446e-84be-964c4ad4b844')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2c69c042-58a4-446e-84be-964c4ad4b844 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "qrels_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGe6BxGCooUb"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nP3FQBGfooUb"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n",
        "\n",
        "feature = tfidf_vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "def retrieve_tf_idf_top_n(query, k):\n",
        "    query_vec = tfidf_vectorizer.transform([query])\n",
        "\n",
        "    results = cosine_similarity(tfidf_matrix,query_vec).flatten()\n",
        "\n",
        "    document_pos_indices = sorted(range(len(results)), key = lambda sub: results[sub])[-k:]\n",
        "    return list(map(lambda x: df_proc.iloc[x][\"doc_id\"], document_pos_indices))"
      ],
      "metadata": {
        "id": "o_i5T9ibySUa"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate TF-IDF retrieval success rate\n",
        "\n",
        "N_Tf_Idf = 500\n",
        "Score = [0]*N_Tf_Idf\n",
        "for i in range(10000):\n",
        "    qrel = qrels_train.iloc[i]\n",
        "    query = qrel[\"text\"]\n",
        "    correct_doc_id = qrel[\"doc_id\"]\n",
        "\n",
        "    tf_idf_doc_ids = retrieve_tf_idf_top_n(query, N_Tf_Idf)\n",
        "\n",
        "    for j, doc_id in enumerate(tf_idf_doc_ids) :\n",
        "        if doc_id == correct_doc_id:\n",
        "            Score[j] +=1\n",
        "print(Score)"
      ],
      "metadata": {
        "id": "ie0MGbV4v6ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(Score[-500:]))\n",
        "print(sum(Score[-100:]))\n",
        "print(sum(Score[-50:]))\n",
        "print(sum(Score[-10:]))"
      ],
      "metadata": {
        "id": "piodDQh7wSfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-07T10:05:12.868856Z",
          "start_time": "2024-01-07T10:05:12.809721Z"
        },
        "id": "SYOayWnxooUc",
        "outputId": "2a083347-e00e-4abf-93f3-248f411bea8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            Word    TF-IDF\n",
            "10769        cam  0.800548\n",
            "34638      killa  0.588214\n",
            "56995       sing  0.042899\n",
            "13152       clap  0.041008\n",
            "65962        uhh  0.018107\n",
            "...          ...       ...\n",
            "24039   foodmart  0.000000\n",
            "24040  foodstamp  0.000000\n",
            "24041      fooey  0.000000\n",
            "24042       foof  0.000000\n",
            "72003        𝑤𝑎𝑠  0.000000\n",
            "\n",
            "[72004 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "doc_vector = tfidf_matrix[0].toarray()\n",
        "#df with words and their tf-idf values\n",
        "df_tfidf = pd.DataFrame(list(zip(feature, doc_vector.flatten())), columns=['Word', 'TF-IDF'])\n",
        "\n",
        "df_tfidf = df_tfidf.sort_values(by='TF-IDF', ascending=False)\n",
        "print(df_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "pdDp4xs4ooUc"
      },
      "source": [
        "# Word2Vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKql958cooUc"
      },
      "outputs": [],
      "source": [
        "# TODO this doesn't work. Remove from final file?\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "print(df_proc.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK43BL8uooUc"
      },
      "outputs": [],
      "source": [
        "# Extract the tokenized lyrics as a list of lists\n",
        "corpus = df_proc['text'].apply(lambda x: x.split()).tolist()\n",
        "\n",
        "# Train Word2Vec model\n",
        "w2v_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liNxRJkRooUd"
      },
      "outputs": [],
      "source": [
        "def average_word_vectors(tokens, model, num_features):\n",
        "    feature_vector = np.zeros((num_features,), dtype=\"float32\")\n",
        "    n_words = 0\n",
        "    for token in tokens:\n",
        "        if token in model.wv:\n",
        "            n_words += 1\n",
        "            feature_vector = np.add(feature_vector, model.wv[token])\n",
        "    if n_words > 0:\n",
        "        feature_vector = np.divide(feature_vector, n_words)\n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "df_proc['doc_vectors'] = [average_word_vectors(tokens, w2v_model, 100) for tokens in corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYWKQ9eDooUd"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#find similar songs to the first one.\n",
        "similarity_matrix = cosine_similarity(df_proc['doc_vectors'].tolist(), [df_proc['doc_vectors'].iloc[0]])\n",
        "similar_songs_indices = np.argsort(similarity_matrix[:, 0])[::-1]\n",
        "\n",
        "top_similar_songs = df_proc.iloc[similar_songs_indices[1:10]][['title', 'text']]\n",
        "print(top_similar_songs)\n",
        "#i am just trying things out rn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y_vKzfiooUd"
      },
      "outputs": [],
      "source": [
        "def generate_training_data(corpus, window_size, vocab_size, word_to_index, length_of_corpus, sample=None):\n",
        "    training_data = []\n",
        "    training_sample_words = []\n",
        "    for i, word in enumerate(corpus):\n",
        "\n",
        "        index_target_word = i\n",
        "        target_word = word\n",
        "        context_words = []\n",
        "\n",
        "        #when target word is the first word\n",
        "        if i == 0:\n",
        "\n",
        "            # trgt_word_index:(0), ctxt_word_index:(1,2)\n",
        "            context_words = [corpus[x] for x in range(i + 1, window_size + 1)]\n",
        "\n",
        "            #when target word is the last word\n",
        "        elif i == len(corpus) - 1:\n",
        "\n",
        "            # trgt_word_index:(9), ctxt_word_index:(8,7), length_of_corpus = 10\n",
        "            context_words = [corpus[x] for x in range(length_of_corpus - 2, length_of_corpus - 2 - window_size, -1)]\n",
        "\n",
        "        #When target word is the middle word\n",
        "        else:\n",
        "\n",
        "            #Before the middle target word\n",
        "            before_target_word_index = index_target_word - 1\n",
        "            for x in range(before_target_word_index, before_target_word_index - window_size, -1):\n",
        "                if x >= 0:\n",
        "                    context_words.extend([corpus[x]])\n",
        "\n",
        "            #After the middle target word\n",
        "            after_target_word_index = index_target_word + 1\n",
        "            for x in range(after_target_word_index, after_target_word_index + window_size):\n",
        "                if x < len(corpus):\n",
        "                    context_words.extend([corpus[x]])\n",
        "\n",
        "        trgt_word_vector, ctxt_word_vector = get_one_hot_vectors(target_word, context_words, vocab_size, word_to_index)\n",
        "        training_data.append([trgt_word_vector, ctxt_word_vector])\n",
        "\n",
        "        if sample is not None:\n",
        "            training_sample_words.append([target_word, context_words])\n",
        "\n",
        "    return training_data, training_sample_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "EPFQmWjJooUe"
      },
      "source": [
        "Based on this code: https://github.com/rahul1728jha/Word2Vec_Implementation/blob/master/Word_2_Vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMTD5y7QooUe"
      },
      "outputs": [],
      "source": [
        "# Define parameters\n",
        "window_size = 5\n",
        "vocab_size = len(your_vocabulary)\n",
        "word_to_index = your_word_to_index_dict\n",
        "length_of_corpus = len(corpus)\n",
        "\n",
        "# Call the function to generate training data\n",
        "training_data, training_sample_words = generate_word_similarity_training_data(corpus, window_size, vocab_size, word_to_index,\n",
        "                                                                              length_of_corpus, sample=None)\n",
        "\n",
        "word2vec_model = Word2Vec(sentences=your_training_data, vector_size=your_vector_size, window=your_window_size, min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uaaLY2booUe"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity_word(word1, word2, model):\n",
        "    vector1 = model.wv[word1]\n",
        "    vector2 = model.wv[word2]\n",
        "    similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
        "    return similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-SXCjykooUe"
      },
      "outputs": [],
      "source": [
        "similarity_score = cosine_similarity_word('word1', 'word2', word2vec_model)\n",
        "print(f\"Similarity between 'word1' and 'word2': {similarity_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Tu1kL4wdooUf"
      },
      "source": [
        "# monoBERT\n",
        "Based on [github.com/veneres/ltr-emb-analysis](https://github.com/veneres/ltr-emb-analysis)"
      ]
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'transformers'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSequenceClassification\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T21:20:32.764753Z",
          "start_time": "2024-01-08T21:20:32.668738Z"
        },
        "id": "F0ygFdssySUi",
        "outputId": "a3652573-8aca-4202-f320-1f9e2233a2a5"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T17:31:33.605995Z",
          "start_time": "2024-01-08T17:31:33.465442Z"
        },
        "id": "G5N3UHamooUf",
        "outputId": "d30f6710-f2fa-43ff-c20f-2b1a3796efcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch_size=4\n",
            "num_epochs=1\n",
            "num_training_steps=100000\n",
            "save_checkpoint_after_steps=1000\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m qrels_dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./subset-qrels.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     23\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "batch_size = 4\n",
        "\n",
        "print(f\"{batch_size=}\")\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "print(f\"{num_epochs=}\")\n",
        "\n",
        "num_training_steps = int(1e5)\n",
        "\n",
        "print(f\"{num_training_steps=}\")\n",
        "\n",
        "save_checkpoint_after_steps = 1000\n",
        "\n",
        "print(f\"{save_checkpoint_after_steps=}\")\n",
        "\n",
        "docs_dataset_path = \"./documents.pkl\"\n",
        "qrels_dataset_path = \"./qrels-train.pkl\"\n",
        "\n",
        "output_dir = \"out\"\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpASDqkZooUf"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ7BcnKAooUf",
        "ExecuteTime": {
          "end_time": "2024-01-08T21:19:34.094309Z",
          "start_time": "2024-01-08T21:19:34.055368Z"
        },
        "outputId": "13d3951c-7724-4cfb-b0dc-8a52de0f02e1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDatasetLoader\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokenizer, documents_path: \u001b[38;5;28mstr\u001b[39m, qrels_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocuments \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(documents_path)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
          ]
        }
      ],
      "source": [
        "class DatasetLoader(Dataset):\n",
        "    def __init__(self, tokenizer, documents_path: str, qrels_path: str):\n",
        "        self.documents = torch.load(documents_path)\n",
        "        self.qrels = torch.load(qrels_path)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qrels)\n",
        "\n",
        "    def get_labels(self):\n",
        "        return self.qrels[:, 2]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        qrel = self.qrels.iloc[idx]\n",
        "\n",
        "        query_text = qrel[0]\n",
        "        doc_id = qrel[1]\n",
        "        relevance = qrel[2]\n",
        "\n",
        "        doc_text = self.documents.loc[self.documents.index == doc_id].iloc[0][\"text\"]\n",
        "\n",
        "        #print(f\"{query_text=}\")\n",
        "        #print(f\"{doc_text=}\")\n",
        "\n",
        "        tokenized_text = self.tokenizer(\n",
        "            query_text,\n",
        "            doc_text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "        tokenized_text = {k: v[0] for k, v in tokenized_text.items()}\n",
        "        #tensor_qid = torch.tensor(int(query_id))\n",
        "        #\"query_id\": tensor_qid,\n",
        "        tensor_did = torch.tensor(int(doc_id))\n",
        "        return {**tokenized_text, \"labels\": relevance, \"doc_id\": tensor_did}\n",
        "\n",
        "gen = torch.Generator()\n",
        "gen.manual_seed(2147483647)\n",
        "\n",
        "train_dataset = DatasetLoader(tokenizer, docs_dataset_path, qrels_dataset_path)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=gen)"
      ]
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_dataloader' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241m.\u001b[39mdataset\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
          ]
        }
      ],
      "source": [
        "train_dataloader.dataset"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T21:19:28.786057Z",
          "start_time": "2024-01-08T21:19:28.272013Z"
        },
        "id": "4Ukye7DeySUi",
        "outputId": "c0988e6f-03e9-48ca-bf36-c12f35a6dec4"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpqgbWoMooUg"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
        "\n",
        "num_warmup_steps = 1e4\n",
        "\n",
        "print(f\"{num_warmup_steps=}\")\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZwjPPLdooUh"
      },
      "outputs": [],
      "source": [
        "model.to(device)\n",
        "\n",
        "model.train()\n",
        "\n",
        "step_n = 0\n",
        "window_loss = []\n",
        "for epoch in range(num_epochs):\n",
        "    with tqdm(train_dataloader, unit=\"batch\", total=min(num_training_steps, len(train_dataloader))) as tepoch:\n",
        "        #print(next(iter(tepoch)))\n",
        "\n",
        "        for batch in tepoch:\n",
        "            tepoch.set_description(f\"Epoch {epoch}\")\n",
        "            batch_forward = {k: v.to(device) for k, v in batch.items() if k not in [\"doc_id\", \"labels\"]}\n",
        "            outputs = model(**batch_forward)\n",
        "            logits = outputs.logits\n",
        "            cel_w = nn.CrossEntropyLoss().to(device)\n",
        "            loss_w = cel_w(logits.to(device), batch[\"labels\"].to(device))\n",
        "            loss_w.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            window_loss.append(loss_w.item())\n",
        "            if len(window_loss) < 100:\n",
        "                tepoch.set_postfix(loss=\"---\")\n",
        "            else:\n",
        "                tepoch.set_postfix(loss=np.mean(window_loss))\n",
        "                window_loss = window_loss[1:]\n",
        "            step_n += 1\n",
        "            if step_n % save_checkpoint_after_steps == 0:\n",
        "                output_dir_step = output_dir/str(step_n)\n",
        "                print(f\"Saving checkpoint in: {output_dir_step}\")\n",
        "                model.save_pretrained(output_dir_step)\n",
        "\n",
        "            if step_n == num_training_steps:\n",
        "                break\n",
        "\n",
        "output_dir_step = output_dir / str(step_n)\n",
        "model.save_pretrained(output_dir_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "VJab32zXooUh"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9G5t4lAooUi"
      },
      "outputs": [],
      "source": [
        "# Retrieval pipeline\n",
        "def retrieve(query, df, model, tokenizer, top_k=10):\n",
        "    query = query.lower()\n",
        "    query = re.sub(re.compile(r\"[^\\w\\s]\"), \"\", query)\n",
        "    query = query.rsplit()\n",
        "    query = \" \".join([ps.stem(token) for token in query if token not in stopwords_en])\n",
        "\n",
        "    # TF-IDF\n",
        "    tf_idf_doc_ids = retrieve_tf_idf_top_n(query, 100) # Measured to contain the correct doc in >20% of cases\n",
        "\n",
        "    # monoBERT\n",
        "    doc_ids_with_score = []\n",
        "    for i, doc_id in enumerate(tf_idf_doc_ids):\n",
        "        doc_text = df.loc[df.index == doc_id].iloc[0][\"text\"]\n",
        "\n",
        "        #print(f\"{query=}\")\n",
        "        #print(f\"{doc_text=}\")\n",
        "\n",
        "        tokenized_text = tokenizer(\n",
        "            query,\n",
        "            doc_text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "        tokenized_text = {k: v.to(device) for k, v in tokenized_text.items()}\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**tokenized_text)\n",
        "            logits = outputs.logits\n",
        "            softmax = nn.Softmax(dim=1)\n",
        "            probs = softmax(logits)\n",
        "            probs = probs[:, 1]\n",
        "            probs = probs.cpu().numpy()\n",
        "            doc_ids_with_score.append((doc_id, probs[0]))\n",
        "\n",
        "        if i % 20 == 0:\n",
        "            print(f\"Compared {i} out of {len(tf_idf_doc_ids)} doc ids via monobert\")\n",
        "\n",
        "    doc_ids_with_score = sorted(doc_ids_with_score, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return [doc_id for doc_id, score in doc_ids_with_score[:top_k]]"
      ]
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# Evaluate using test set\n",
        "hitrate_sum = 0\n",
        "mrr_sum = 0\n",
        "precision_sum = 0\n",
        "\n",
        "# Can't compare against the whole test set, that would take too long\n",
        "testing_range = 300\n",
        "for i in range(testing_range):\n",
        "    query = qrels_test.iloc[i][\"text\"]\n",
        "    doc_id = qrels_test.iloc[i][\"doc_id\"]\n",
        "    relevance = qrels_test.iloc[i][\"relevance\"]\n",
        "\n",
        "    top_similar_songs = retrieve(query, df_proc, model, tokenizer, top_k=10)\n",
        "\n",
        "    if doc_id in top_similar_songs:\n",
        "        hitrate_sum += 1\n",
        "        mrr_sum += 1 / (top_similar_songs.index(doc_id) + 1)\n",
        "        precision_sum += 1 / len(top_similar_songs)\n",
        "\n",
        "\n",
        "mean_hitrate = hitrate_sum / testing_range\n",
        "mean_mrr = mrr_sum / testing_range\n",
        "mean_precision = precision_sum / testing_range"
      ],
      "metadata": {
        "id": "0_BQEmMNySUj"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "print(f\"mean_hitrate: {(mean_hitrate * 100):.2f}%\")\n",
        "print(f\"mean_mrr: {mean_mrr:.4f}%\")\n",
        "print(f\"mean_precision: {(mean_precision * 100):.2f}%\")"
      ],
      "metadata": {
        "id": "785y5raPySUj"
      },
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}