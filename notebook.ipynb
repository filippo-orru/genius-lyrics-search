{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.6.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: swifter in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Collecting opendatasets\n",
      "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.6.1.tar.gz (84 kB)\n",
      "Requirement already satisfied: psutil>=5.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from swifter) (5.8.0)\n",
      "Requirement already satisfied: dask[dataframe]>=2.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from swifter) (2021.4.0)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from swifter) (1.2.4)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (5.4.1)\n",
      "Requirement already satisfied: toolz>=0.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (0.11.1)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (1.6.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (1.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->swifter) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->swifter) (2021.1)\n",
      "Requirement already satisfied: locket in c:\\programdata\\anaconda3\\lib\\site-packages\\locket-0.2.1-py3.8.egg (from partd>=0.3.10->dask[dataframe]>=2.10.0->swifter) (0.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.0.0->swifter) (1.15.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (2020.12.5)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (2.25.1)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-8.0.1-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: urllib3 in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (1.26.4)\n",
      "Requirement already satisfied: bleach in c:\\programdata\\anaconda3\\lib\\site-packages (from kaggle->opendatasets) (3.3.0)\n",
      "Requirement already satisfied: webencodings in c:\\programdata\\anaconda3\\lib\\site-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from bleach->kaggle->opendatasets) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->bleach->kaggle->opendatasets) (2.4.7)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kaggle->opendatasets) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kaggle->opendatasets) (4.0.0)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.6.1-py3-none-any.whl size=111907 sha256=d3e625929f0c4a3c2a806b7a07314d0f69e4ddcf331571dd7ba906b6734302ce\n",
      "  Stored in directory: c:\\users\\marc\\appdata\\local\\pip\\cache\\wheels\\56\\e5\\e8\\da57097e519eca5372e1e4bd7d9d3e9fc2743c967b343c6a5b\n",
      "Successfully built kaggle\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle, opendatasets\n",
      "Successfully installed kaggle-1.6.1 opendatasets-0.1.22 python-slugify-8.0.1 text-unidecode-1.3\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk swifter opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T09:59:58.618452Z",
     "start_time": "2024-01-07T09:59:58.607410Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import swifter\n",
    "import torch\n",
    "import opendatasets as od\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Download Data\n",
    "\n",
    "1. Option: download the full dataset and select some percentage. You will need to enter your username and kaggle API token when prompted. You can create an API token at https://www.kaggle.com > settings > API > Create New API Token.\n",
    "\n",
    "2. Option: load preprocessed data and qrels (you can skip the section for preprocessing document and generating queries below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username: "
     ]
    }
   ],
   "source": [
    "# OPTION 1\n",
    "dataset = 'https://www.kaggle.com/datasets/carlosgdcj/genius-song-lyrics-with-language-information/'\n",
    "# Download the data set using opendatasets\n",
    "od.download(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't work with a 9GB file, so read 10% of the data at random\n",
    "p = 0.1\n",
    "# keep the header, then take only p% of lines\n",
    "df = pd.read_csv(\n",
    "    \"./genius-song-lyrics-with-language-information/song_lyrics.csv\",\n",
    "    header=0,\n",
    "    skiprows=lambda i: i > 0 and random.random() > p\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T12:31:34.470187Z",
     "start_time": "2024-01-08T12:31:34.373535Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# OPTION 2Load preprocessed data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# OPTION 2\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve('https://f002.backblazeb2.com/file/ffactory-public/documents.pkl', 'documents.pkl')\n",
    "urllib.request.urlretrieve('https://f002.backblazeb2.com/file/ffactory-public/qrels-train.pkl', 'qrels-train.pkl')\n",
    "urllib.request.urlretrieve('https://f002.backblazeb2.com/file/ffactory-public/qrels-test.pkl', 'qrels-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = torch.load('documents.pkl')\n",
    "qrels_train = torch.load('qrels-train.pkl')\n",
    "qrels_test = torch.load('qrels-test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:00:04.750355Z",
     "start_time": "2024-01-07T10:00:04.744511Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ffactory/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ffactory/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:17:54.823173Z",
     "start_time": "2024-01-07T10:17:53.241227Z"
    }
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "\n",
    "# Select only english songs and songs which contain verses (newlines)\n",
    "df = df[df[\"language\"] == \"en\"]\n",
    "df = df[df[\"lyrics\"].str.contains(\"\\n\")]\n",
    "\n",
    "# Select columns we care about\n",
    "df = df[[\"title\", \"lyrics\", \"views\", \"id\"]]\n",
    "df.rename(columns={\"id\": \"doc_id\"}, inplace=True)\n",
    "df.set_index(\"doc_id\", inplace=True)\n",
    "\n",
    "df_proc = df.copy()\n",
    "df_proc.rename(columns={\"lyrics\": \"text\"}, inplace=True)\n",
    "\n",
    "# Convert to lowercase\n",
    "df_proc[\"text\"] = df_proc[\"text\"].str.lower()\n",
    "# Remove any non-alphanumeric / whitespace characters\n",
    "df_proc[\"text\"] = df_proc[\"text\"].str.replace(re.compile(r\"[^\\w\\s]\"), \"\", regex=True)\n",
    "# Remove newlines\n",
    "df_proc[\"text\"] = df_proc[\"text\"].str.replace(\"\\n\", \" \", regex=False)\n",
    "# Remove text between square brackets\n",
    "df_proc[\"text\"] = df_proc[\"text\"].str.replace(re.compile(r\"\\[.{0,100}\\]\"), \"\", regex=True)\n",
    "# Split text into words\n",
    "df_proc[\"text\"] = df_proc[\"text\"].str.rsplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:00:01.453221Z",
     "start_time": "2024-01-07T10:00:01.430381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of songs: 12295\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "      <th>artist</th>\n",
       "      <th>year</th>\n",
       "      <th>views</th>\n",
       "      <th>features</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>id</th>\n",
       "      <th>language_cld3</th>\n",
       "      <th>language_ft</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Killa Cam</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>173166</td>\n",
       "      <td>{\"Cam\\\\'ron\",\"Opera Steve\"}</td>\n",
       "      <td>[Chorus: Opera Steve &amp; Cam'ron]\\nKilla Cam, Ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I Live</td>\n",
       "      <td>rap</td>\n",
       "      <td>JAY-Z</td>\n",
       "      <td>1996</td>\n",
       "      <td>468624</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Produced by Irv Gotti]\\n\\n[Intro]\\nYeah, hah,...</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forgive Me Father</td>\n",
       "      <td>rap</td>\n",
       "      <td>Fabolous</td>\n",
       "      <td>2003</td>\n",
       "      <td>4743</td>\n",
       "      <td>{}</td>\n",
       "      <td>Maybe cause I'm eatin\\nAnd these bastards fien...</td>\n",
       "      <td>4</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Down and Out</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>144404</td>\n",
       "      <td>{\"Cam\\\\'ron\",\"Kanye West\",\"Syleena Johnson\"}</td>\n",
       "      <td>[Produced by Kanye West and Brian Miller]\\n\\n[...</td>\n",
       "      <td>5</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fly In</td>\n",
       "      <td>rap</td>\n",
       "      <td>Lil Wayne</td>\n",
       "      <td>2005</td>\n",
       "      <td>78271</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Intro]\\nSo they ask me\\n\"Young boy\\nWhat you ...</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               title  tag     artist  year   views  \\\n",
       "0          Killa Cam  rap    Cam'ron  2004  173166   \n",
       "1         Can I Live  rap      JAY-Z  1996  468624   \n",
       "2  Forgive Me Father  rap   Fabolous  2003    4743   \n",
       "3       Down and Out  rap    Cam'ron  2004  144404   \n",
       "4             Fly In  rap  Lil Wayne  2005   78271   \n",
       "\n",
       "                                       features  \\\n",
       "0                   {\"Cam\\\\'ron\",\"Opera Steve\"}   \n",
       "1                                            {}   \n",
       "2                                            {}   \n",
       "3  {\"Cam\\\\'ron\",\"Kanye West\",\"Syleena Johnson\"}   \n",
       "4                                            {}   \n",
       "\n",
       "                                              lyrics  id language_cld3  \\\n",
       "0  [Chorus: Opera Steve & Cam'ron]\\nKilla Cam, Ki...   1            en   \n",
       "1  [Produced by Irv Gotti]\\n\\n[Intro]\\nYeah, hah,...   3            en   \n",
       "2  Maybe cause I'm eatin\\nAnd these bastards fien...   4            en   \n",
       "3  [Produced by Kanye West and Brian Miller]\\n\\n[...   5            en   \n",
       "4  [Intro]\\nSo they ask me\\n\"Young boy\\nWhat you ...   6            en   \n",
       "\n",
       "  language_ft language  \n",
       "0          en       en  \n",
       "1          en       en  \n",
       "2          en       en  \n",
       "3          en       en  \n",
       "4          en       en  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"number of english songs: {len(df_proc)}\")\n",
    "df_proc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:00:47.261930Z",
     "start_time": "2024-01-07T10:00:11.500207Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8810b32d7f5944c7bcb464d2b6805c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/12064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove stopwords and stem tokens\n",
    "def remove_stopwords_and_stem(tokens):\n",
    "    return \" \".join([ps.stem(token) for token in tokens if token not in stopwords_en])\n",
    "\n",
    "\n",
    "df_proc[\"text\"] = df_proc[\"text\"].swifter.apply(remove_stopwords_and_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:00:51.886296Z",
     "start_time": "2024-01-07T10:00:51.862829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"[Intro: Lil Wayne]\\nHaha\\nUh-huh\\nNo homo (Young Mula, baby!)\\nI say, he's so sweet, make her wanna lick\",\n",
       " 'intro lil wayn haha uhhuh homo young mula babi say he sweet make wanna lick wrapper remix babi vers ')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the original lyrics with the tokenized lyrics\n",
    "(df.iloc[5][\"lyrics\"][0:100], df_proc.iloc[5][\"text\"][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:00:56.242240Z",
     "start_time": "2024-01-07T10:00:56.201640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>views</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Killa Cam</td>\n",
       "      <td>173166</td>\n",
       "      <td>choru opera steve camron killa cam killa cam c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I Live</td>\n",
       "      <td>468624</td>\n",
       "      <td>produc irv gotti intro yeah hah yeah rocafella...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forgive Me Father</td>\n",
       "      <td>4743</td>\n",
       "      <td>mayb caus im eatin bastard fiend grub carri pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Down and Out</td>\n",
       "      <td>144404</td>\n",
       "      <td>produc kany west brian miller intro camron kan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fly In</td>\n",
       "      <td>78271</td>\n",
       "      <td>intro ask young boy gon second time around gon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               title   views  \\\n",
       "0          Killa Cam  173166   \n",
       "1         Can I Live  468624   \n",
       "2  Forgive Me Father    4743   \n",
       "3       Down and Out  144404   \n",
       "4             Fly In   78271   \n",
       "\n",
       "                                                text  \n",
       "0  choru opera steve camron killa cam killa cam c...  \n",
       "1  produc irv gotti intro yeah hah yeah rocafella...  \n",
       "2  mayb caus im eatin bastard fiend grub carri pu...  \n",
       "3  produc kany west brian miller intro camron kan...  \n",
       "4  intro ask young boy gon second time around gon...  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_proc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:01:03.954364Z",
     "start_time": "2024-01-07T10:01:03.704188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Killa Cam</td>\n",
       "      <td>choru opera steve camron killa cam killa cam c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I Live</td>\n",
       "      <td>produc irv gotti intro yeah hah yeah rocafella...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forgive Me Father</td>\n",
       "      <td>mayb caus im eatin bastard fiend grub carri pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Down and Out</td>\n",
       "      <td>produc kany west brian miller intro camron kan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fly In</td>\n",
       "      <td>intro ask young boy gon second time around gon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               title                                               text\n",
       "0          Killa Cam  choru opera steve camron killa cam killa cam c...\n",
       "1         Can I Live  produc irv gotti intro yeah hah yeah rocafella...\n",
       "2  Forgive Me Father  mayb caus im eatin bastard fiend grub carri pu...\n",
       "3       Down and Out  produc kany west brian miller intro camron kan...\n",
       "4             Fly In  intro ask young boy gon second time around gon..."
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save as a pickle file\n",
    "torch.save(df_proc[[\"title\", \"text\"]], 'documents.pkl')\n",
    "\n",
    "# Test if the pickle file is saved correctly\n",
    "df_reloaded = torch.load('documents.pkl')\n",
    "df_reloaded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:01:24.590311Z",
     "start_time": "2024-01-07T10:01:24.579912Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions to Select Verses\n",
    "def getFirstVerses(lyricsString, amount):\n",
    "    verseList = re.split('\\n', lyricsString)\n",
    "    FinalList = [i for i in verseList if (len(i) > 1 and i[0] != '[')]\n",
    "    return \" \".join(FinalList[:amount])\n",
    "\n",
    "\n",
    "def getFirstVersesOfChorus(lyricsString, amount):\n",
    "    List = re.split('\\n', lyricsString)\n",
    "    verseList = [i for i in List if len(i) > 1]\n",
    "    for i in range(len(verseList)):\n",
    "        if \"[Chorus\" in verseList[i] or \"[Hook\" in verseList[i]:\n",
    "            return \" \".join(verseList[i + 1:i + amount + 1])\n",
    "    return getFirstVerses(lyricsString, amount)\n",
    "\n",
    "\n",
    "def getRandomVerses(lyricsString, amount):\n",
    "    verseList = re.split('\\n', lyricsString)\n",
    "    FinalList = [i for i in verseList if (len(i) > 1 and i[0] != '[')]\n",
    "    rd = random.randint(0, len(FinalList) - amount)\n",
    "    return \" \".join(FinalList[rd:rd + amount])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:02:13.193171Z",
     "start_time": "2024-01-07T10:02:13.185882Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions to Degrade message\n",
    "\n",
    "#Function to create typo by neighbouring letter\n",
    "NeighbouringKeys = {\n",
    "    'q': \"qwas\",\n",
    "    'w': \"qwase\",\n",
    "    'e': \"wsedr\",\n",
    "    'r': \"edrft\",\n",
    "    't': \"rftgy\",\n",
    "    'y': \"tgyhu\",\n",
    "    'u': \"yhuji\",\n",
    "    'i': \"ujiko\",\n",
    "    'o': \"ikolp\",\n",
    "    'p': \"olp\",\n",
    "    'a': \"qwasz\",\n",
    "    's': \"wazsxed\",\n",
    "    'd': \"sxedcrf\",\n",
    "    'f': \"dcrfvtg\",\n",
    "    'g': \"fvtgbyh\",\n",
    "    'h': \"gbyhnuj\",\n",
    "    'j': \"hnujmik\",\n",
    "    'k': \"jmikol\",\n",
    "    'l': \"kolp\",\n",
    "    'z': \"azsx\",\n",
    "    'x': \"zsxdc\",\n",
    "    'c': \"xdcfv\",\n",
    "    'v': \"cfvgb\",\n",
    "    'b': \"vgbhn\",\n",
    "    'n': \"bhnjm\",\n",
    "    'm': \"njmk\"\n",
    "}\n",
    "\n",
    "englishLetters = NeighbouringKeys.keys()\n",
    "\n",
    "\n",
    "def typos(text, prob=0.01):\n",
    "    resultingText = \"\"\n",
    "\n",
    "    for letter in text:\n",
    "        if not letter in englishLetters:\n",
    "            newLetter = letter\n",
    "        else:\n",
    "            if random.random() < prob:\n",
    "                newLetter = random.choice(NeighbouringKeys[letter])\n",
    "            else:\n",
    "                newLetter = letter\n",
    "        resultingText += newLetter\n",
    "\n",
    "    return resultingText\n",
    "\n",
    "\n",
    "#Function to (maybe) invert 2 adjacent letters (do force=True to force it to happen)\n",
    "def invertAdjacentLetters(text, force=False):\n",
    "    rd = random.randint(0, len(text) - 2)\n",
    "    if not force:\n",
    "        if text[rd] in englishLetters and text[rd + 1] in englishLetters:\n",
    "            return text[:rd] + text[rd + 1] + text[rd] + text[rd + 2:]\n",
    "        else:\n",
    "            return text\n",
    "    else:\n",
    "        while not (text[rd] in englishLetters and text[rd + 1] in englishLetters):\n",
    "            rd = random.randint(0, len(text) - 2)\n",
    "        return text[:rd] + text[rd + 1] + text[rd] + text[rd + 2:]\n",
    "\n",
    "\n",
    "#Function to (maybe) remove a letter (do force=True to force it to happen)\n",
    "def removeLetter(text, force=False):\n",
    "    rd = random.randint(0, len(text) - 1)\n",
    "    if not force:\n",
    "        if text[rd] in englishLetters:\n",
    "            return text[:rd] + text[rd + 1:]\n",
    "        else:\n",
    "            return text\n",
    "    else:\n",
    "        while not (text[rd] in englishLetters):\n",
    "            rd = random.randint(0, len(text) - 1)\n",
    "        return text[:rd] + text[rd + 1:]\n",
    "\n",
    "\n",
    "#Function to (maybe) double a letter (do force=True to force it to happen)\n",
    "def doubleLetter(text, force=False):\n",
    "    rd = random.randint(0, len(text) - 1)\n",
    "    if not force:\n",
    "        if text[rd] in englishLetters:\n",
    "            return text[:rd + 1] + text[rd] + text[rd + 1:]\n",
    "        else:\n",
    "            return text\n",
    "    else:\n",
    "        while not (text[rd] in englishLetters):\n",
    "            rd = random.randint(0, len(text) - 1)\n",
    "        return text[:rd + 1] + text[rd] + text[rd + 1:]\n",
    "\n",
    "\n",
    "CommonMisspelling = {\n",
    "    \"absence\": [\"absense\", \"absentse\", \"abcense\", \"absance\"],\n",
    "    \"acceptable\": [\"acceptible\"],\n",
    "    \"their\": [\"there\", \"they're\"],\n",
    "    \"there\": [\"their\", \"they're\"],\n",
    "    \"they're\": [\"their\", \"there\"],\n",
    "    \"your\": [\"you're\"],\n",
    "    \"you're\": [\"your\"]\n",
    "}\n",
    "\n",
    "\n",
    "# Add a common misspelling\n",
    "def addCommonMisspell(text):\n",
    "    for word in CommonMisspelling.keys():\n",
    "        if word in text:\n",
    "            return text.replace(word, random.choice(CommonMisspelling[word]))\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:28:55.358663Z",
     "start_time": "2024-01-07T10:28:55.337489Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_qrels(df, n):\n",
    "    # Add 'weight' column\n",
    "    max_views = max(df[\"views\"])\n",
    "    df[\"weight\"] = (df[\"views\"] / max_views) ** 0.5 * 0.5 + 0.1\n",
    "\n",
    "    df_sampled = df.sample(n // 2, weights='weight')\n",
    "\n",
    "    def generate_positive_qrel(document):\n",
    "        text = document['lyrics']\n",
    "        rd = random.random()\n",
    "        if rd < 0.6:\n",
    "            query = getFirstVersesOfChorus(text, random.randint(1, 2))\n",
    "        elif rd < 0.9:\n",
    "            query = getFirstVerses(text, random.randint(1, 2))\n",
    "        else:\n",
    "            query = getRandomVerses(text, random.randint(1, 2))\n",
    "\n",
    "        if random.randint(0, 3) == 0:\n",
    "            query = addCommonMisspell(query)\n",
    "        query = typos(query)\n",
    "\n",
    "        for j in range(len(query)):\n",
    "            rand = random.randint(0, 50)\n",
    "            if rand == 0:\n",
    "                query = invertAdjacentLetters(query)\n",
    "            elif rand == 1:\n",
    "                query = removeLetter(query)\n",
    "            if rand == 2:\n",
    "                query = doubleLetter(query)\n",
    "\n",
    "        doc_id = document.name\n",
    "        return pd.Series([query, doc_id, 1], index=['text', 'doc_id', 'relevance'])\n",
    "\n",
    "    def generate_negative_qrel(positive_qrel):\n",
    "        original_doc_id = positive_qrel['doc_id']\n",
    "        negative_doc_id = original_doc_id\n",
    "        while negative_doc_id == original_doc_id:\n",
    "            negative_doc_id = df.sample(1).iloc[0].name\n",
    "\n",
    "        return pd.Series([positive_qrel['text'], negative_doc_id, 0])\n",
    "\n",
    "    positive_qrels = df_sampled.apply(generate_positive_qrel, axis=1, result_type='expand')\n",
    "    negative_qrels = positive_qrels.apply(generate_negative_qrel, axis=1, result_type='broadcast')\n",
    "\n",
    "    return pd.concat([positive_qrels, negative_qrels]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:28:57.706140Z",
     "start_time": "2024-01-07T10:28:57.324998Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save qrels as pickle file\n",
    "total_qrels = 1000\n",
    "#total_qrels = 100000\n",
    "qrels_train = generate_qrels(df, int(total_qrels * 0.9))\n",
    "torch.save(qrels_train, 'qrels-train.pkl')\n",
    "\n",
    "qrels_test = generate_qrels(df, int(total_qrels * 0.1))\n",
    "torch.save(qrels_test, 'qrels-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intro: Syn</td>\n",
       "      <td>7865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brand Nubian baby, heere to lip it again And y...</td>\n",
       "      <td>11134</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From a nickel and dime ass nigga</td>\n",
       "      <td>8475</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Verse 1 (Killa Tay) We pump lugs, and punk thu...</td>\n",
       "      <td>11702</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(I) Fuck with your osul likee ethher</td>\n",
       "      <td>230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>All in together now (no, now, now) What are yo...</td>\n",
       "      <td>9078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Bling bling, every ttime I come around your ci...</td>\n",
       "      <td>1016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Ha Huh mann Im trippiin out right now</td>\n",
       "      <td>2695</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>All the ganggstas they gon' ride to tis</td>\n",
       "      <td>1822</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>I could tell you wanna grab the mic because yo...</td>\n",
       "      <td>10079</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text doc_id relevance\n",
       "0                                            Intro: Syn   7865         1\n",
       "1     Brand Nubian baby, heere to lip it again And y...  11134         1\n",
       "2                      From a nickel and dime ass nigga   8475         1\n",
       "3     Verse 1 (Killa Tay) We pump lugs, and punk thu...  11702         1\n",
       "4                  (I) Fuck with your osul likee ethher    230         1\n",
       "...                                                 ...    ...       ...\n",
       "9995  All in together now (no, now, now) What are yo...   9078         0\n",
       "9996  Bling bling, every ttime I come around your ci...   1016         0\n",
       "9997              Ha Huh mann Im trippiin out right now   2695         0\n",
       "9998            All the ganggstas they gon' ride to tis   1822         0\n",
       "9999  I could tell you wanna grab the mic because yo...  10079         0\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_proc['text'])\n",
    "\n",
    "feature = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"baby\"\n",
    "query_vec = tfidf_vectorizer.transform([query])\n",
    "\n",
    "results = cosine_similarity(tfidf_matrix,query_vec).flatten()\n",
    "\n",
    "res = sorted(range(len(results)), key = lambda sub: results[sub])[-10:]\n",
    "\n",
    "for i in res[::-1]:\n",
    "    print(df[\"title\"][i],\"--\",results[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:05:12.868856Z",
     "start_time": "2024-01-07T10:05:12.809721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Word    TF-IDF\n",
      "10769        cam  0.800548\n",
      "34638      killa  0.588214\n",
      "56995       sing  0.042899\n",
      "13152       clap  0.041008\n",
      "65962        uhh  0.018107\n",
      "...          ...       ...\n",
      "24039   foodmart  0.000000\n",
      "24040  foodstamp  0.000000\n",
      "24041      fooey  0.000000\n",
      "24042       foof  0.000000\n",
      "72003        ð‘¤ð‘Žð‘   0.000000\n",
      "\n",
      "[72004 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "doc_vector = tfidf_matrix[0].toarray()\n",
    "#df with words and their tf-idf values\n",
    "df_tfidf = pd.DataFrame(list(zip(feature, doc_vector.flatten())), columns=['Word', 'TF-IDF'])\n",
    "\n",
    "df_tfidf = df_tfidf.sort_values(by='TF-IDF', ascending=False)\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO this doesn't work. Remove from final file?\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "print(df_proc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract the tokenized lyrics as a list of lists\n",
    "corpus = df_proc['text'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def average_word_vectors(tokens, model, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float32\")\n",
    "    n_words = 0\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            n_words += 1\n",
    "            feature_vector = np.add(feature_vector, model.wv[token])\n",
    "    if n_words > 0:\n",
    "        feature_vector = np.divide(feature_vector, n_words)\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "df_proc['doc_vectors'] = [average_word_vectors(tokens, w2v_model, 100) for tokens in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#find similar songs to the first one.\n",
    "similarity_matrix = cosine_similarity(df_proc['doc_vectors'].tolist(), [df_proc['doc_vectors'].iloc[0]])\n",
    "similar_songs_indices = np.argsort(similarity_matrix[:, 0])[::-1]\n",
    "\n",
    "top_similar_songs = df_proc.iloc[similar_songs_indices[1:10]][['title', 'text']]\n",
    "print(top_similar_songs)\n",
    "#i am just trying things out rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_training_data(corpus, window_size, vocab_size, word_to_index, length_of_corpus, sample=None):\n",
    "    training_data = []\n",
    "    training_sample_words = []\n",
    "    for i, word in enumerate(corpus):\n",
    "\n",
    "        index_target_word = i\n",
    "        target_word = word\n",
    "        context_words = []\n",
    "\n",
    "        #when target word is the first word\n",
    "        if i == 0:\n",
    "\n",
    "            # trgt_word_index:(0), ctxt_word_index:(1,2)\n",
    "            context_words = [corpus[x] for x in range(i + 1, window_size + 1)]\n",
    "\n",
    "            #when target word is the last word\n",
    "        elif i == len(corpus) - 1:\n",
    "\n",
    "            # trgt_word_index:(9), ctxt_word_index:(8,7), length_of_corpus = 10\n",
    "            context_words = [corpus[x] for x in range(length_of_corpus - 2, length_of_corpus - 2 - window_size, -1)]\n",
    "\n",
    "        #When target word is the middle word\n",
    "        else:\n",
    "\n",
    "            #Before the middle target word\n",
    "            before_target_word_index = index_target_word - 1\n",
    "            for x in range(before_target_word_index, before_target_word_index - window_size, -1):\n",
    "                if x >= 0:\n",
    "                    context_words.extend([corpus[x]])\n",
    "\n",
    "            #After the middle target word\n",
    "            after_target_word_index = index_target_word + 1\n",
    "            for x in range(after_target_word_index, after_target_word_index + window_size):\n",
    "                if x < len(corpus):\n",
    "                    context_words.extend([corpus[x]])\n",
    "\n",
    "        trgt_word_vector, ctxt_word_vector = get_one_hot_vectors(target_word, context_words, vocab_size, word_to_index)\n",
    "        training_data.append([trgt_word_vector, ctxt_word_vector])\n",
    "\n",
    "        if sample is not None:\n",
    "            training_sample_words.append([target_word, context_words])\n",
    "\n",
    "    return training_data, training_sample_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Based on this code: https://github.com/rahul1728jha/Word2Vec_Implementation/blob/master/Word_2_Vec.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "window_size = 5\n",
    "vocab_size = len(your_vocabulary)\n",
    "word_to_index = your_word_to_index_dict\n",
    "length_of_corpus = len(corpus)\n",
    "\n",
    "# Call the function to generate training data\n",
    "training_data, training_sample_words = generate_word_similarity_training_data(corpus, window_size, vocab_size, word_to_index,\n",
    "                                                                              length_of_corpus, sample=None)\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=your_training_data, vector_size=your_vector_size, window=your_window_size, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_word(word1, word2, model):\n",
    "    vector1 = model.wv[word1]\n",
    "    vector2 = model.wv[word2]\n",
    "    similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similarity_score = cosine_similarity_word('word1', 'word2', word2vec_model)\n",
    "print(f\"Similarity between 'word1' and 'word2': {similarity_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# monoBERT\n",
    "Based on [github.com/veneres/ltr-emb-analysis](https://github.com/veneres/ltr-emb-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T17:31:33.605995Z",
     "start_time": "2024-01-08T17:31:33.465442Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size=4\n",
      "num_epochs=1\n",
      "num_training_steps=100000\n",
      "save_checkpoint_after_steps=1000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m qrels_dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./subset-qrels.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     23\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "print(f\"{batch_size=}\")\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "print(f\"{num_epochs=}\")\n",
    "\n",
    "num_training_steps = int(1e5)\n",
    "\n",
    "print(f\"{num_training_steps=}\")\n",
    "\n",
    "save_checkpoint_after_steps = 1000\n",
    "\n",
    "print(f\"{save_checkpoint_after_steps=}\")\n",
    "\n",
    "docs_dataset_path = \"./documents.pkl\"\n",
    "qrels_dataset_path = \"./qrels-train.pkl\"\n",
    "\n",
    "output_dir = \"out\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DatasetLoader(Dataset):\n",
    "    def __init__(self, tokenizer, documents_path: str, qrels_path: str):\n",
    "        self.documents = torch.load(documents_path)\n",
    "        self.qrels = torch.load(qrels_path)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qrels)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.qrels[:, 2]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        qrel = self.qrels.iloc[idx]\n",
    "\n",
    "        query_tokens = qrel[0]\n",
    "        doc_id = qrel[1]\n",
    "        relevance = qrel[2]\n",
    "\n",
    "        rel_docs_tokens = self.documents.loc[self.documents.index == doc_id].iloc[0][\"text\"]\n",
    "\n",
    "        #print(f\"{query_tokens=}\")\n",
    "        #print(f\"{rel_docs_tokens=}\")\n",
    "\n",
    "        tokenized_text = self.tokenizer(\n",
    "            query_tokens,\n",
    "            rel_docs_tokens,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        tokenized_text = {k: v[0] for k, v in tokenized_text.items()}\n",
    "        #tensor_qid = torch.tensor(int(query_id))\n",
    "        #\"query_id\": tensor_qid, \n",
    "        tensor_did = torch.tensor(int(doc_id))\n",
    "        return {**tokenized_text, \"labels\": relevance, \"doc_id\": tensor_did}\n",
    "\n",
    "gen = torch.Generator()\n",
    "gen.manual_seed(2147483647)\n",
    "\n",
    "train_dataset = DatasetLoader(tokenizer, docs_dataset_path, qrels_dataset_path)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "num_warmup_steps = 1e4\n",
    "\n",
    "print(f\"{num_warmup_steps=}\")\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "step_n = 0\n",
    "window_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    with tqdm(train_dataloader, unit=\"batch\", total=min(num_training_steps, len(train_dataloader))) as tepoch:\n",
    "        #print(next(iter(tepoch)))\n",
    "\n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            batch_forward = {k: v.to(device) for k, v in batch.items() if k not in [\"doc_id\", \"labels\"]}\n",
    "            outputs = model(**batch_forward)\n",
    "            logits = outputs.logits\n",
    "            cel_w = nn.CrossEntropyLoss().to(device)\n",
    "            loss_w = cel_w(logits.to(device), batch[\"labels\"].to(device))\n",
    "            loss_w.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            window_loss.append(loss_w.item())\n",
    "            if len(window_loss) < 100:\n",
    "                tepoch.set_postfix(loss=\"---\")\n",
    "            else:\n",
    "                tepoch.set_postfix(loss=np.mean(window_loss))\n",
    "                window_loss = window_loss[1:]\n",
    "            step_n += 1\n",
    "            if step_n % save_checkpoint_after_steps == 0:\n",
    "                output_dir_step = output_dir/str(step_n)\n",
    "                print(f\"Saving checkpoint in: {output_dir_step}\")\n",
    "                model.save_pretrained(output_dir_step)\n",
    "\n",
    "            if step_n == num_training_steps:\n",
    "                break\n",
    "\n",
    "output_dir_step = output_dir / str(step_n)\n",
    "model.save_pretrained(output_dir_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieval pipeline\n",
    "def retrieve(query, df, model, tokenizer, top_k=10):\n",
    "    query = query.lower()\n",
    "    query = query.replace(re.compile(r\"[^\\w\\s]\"), \"\", regex=True)\n",
    "    query = query.rsplit()\n",
    "    query = \" \".join([ps.stem(token) for token in query if token not in stopwords_en])\n",
    "\n",
    "    # TF-IDF\n",
    "    query_vector = tfidf_vectorizer.transform([query]).toarray()\n",
    "\n",
    "    # monoBERT\n",
    "    for\n",
    "        query_tokens = tokenizer(query, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "    query_tokens = {k: v[0].to(device) for k, v in query_tokens.items()}\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**query_tokens)\n",
    "        logits = outputs.logits\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        probs = softmax(logits)\n",
    "        probs = probs[:, 1]\n",
    "        probs = probs.cpu().numpy()\n",
    "\n",
    "    # \n",
    "\n",
    "    return top_similar_songs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
